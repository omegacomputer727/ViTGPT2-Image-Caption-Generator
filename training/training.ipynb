{\n "cells": [\n  {\n   "cell_type": "code",\n   "execution_count": null,\n   "metadata": {},\n   "outputs": [],\n   "source": [\n    "!pip install datasets\n",\n    "!pip install transformers\n",\n    "!pip install tensorflow==2.15\n",\n    "!pip install evaluate\n",\n    "!pip install accelerate\n"\n   ]\n  },\n  {\n   "cell_type": "code",\n   "execution_count": null,\n   "metadata": {},\n   "outputs": [],\n   "source": [\n    "from google.colab import drive\n",\n    "drive.mount('/content/drive')"\n   ]\n  },\n  {\n   "cell_type": "code",\n   "execution_count": null,\n   "metadata": {},\n   "outputs": [],\n   "source": [\n    "import os\n",\n    "import datasets\n",\n    "from transformers import VisionEncoderDecoderModel, AutoFeatureExtractor,AutoTokenizer\n",\n    "os.environ["WANDB_DISABLED"] = "true""\n   ]\n  },\n  {\n   "cell_type": "code",\n   "execution_count": null,\n   "metadata": {},\n   "outputs": [],\n   "source": [\n    "import nltk\n",\n    "try:\n",\n    "    nltk.data.find("tokenizers/punkt")\n",\n    "except (LookupError, OSError):\n",\n    "    nltk.download("punkt", quiet=True)"\n   ]\n  },\n  {\n   "cell_type": "code",\n   "execution_count": null,\n   "metadata": {},\n   "outputs": [],\n   "source": [\n    "image_encoder_model = "google/vit-base-patch16-224-in21k"\n",\n    "text_decode_model = "gpt2"\n",\n    "\n",\n    "model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n",\n    "    image_encoder_model, text_decode_model)"\n   ]\n  },\n  {\n   "cell_type": "code",\n   "execution_count": null,\n   "metadata": {},\n   "outputs": [],\n   "source": [\n    "feature_extractor = AutoFeatureExtractor.from_pretrained(image_encoder_model)\n",\n    "tokenizer = AutoTokenizer.from_pretrained(text_decode_model)"\n   ]\n  },\n  {\n   "cell_type": "code",\n   "execution_count": null,\n   "metadata": {},\n   "outputs": [],\n   "source": [\n    "tokenizer.pad_token = tokenizer.eos_token\n",\n    "model.config.eos_token_id = tokenizer.eos_token_id\n",\n    "model.config.decoder_start_token_id = tokenizer.bos_token_id\n",\n    "model.config.pad_token_id = tokenizer.pad_token_id"\n   ]\n  },\n  {\n   "cell_type": "code",\n   "execution_count": null,\n   "metadata": {},\n   "outputs": [],\n   "source": [\n    "output_dir = "/content"\n",\n    "model.save_pretrained(output_dir)\n",\n    "feature_extractor.save_pretrained(output_dir)\n",\n    "tokenizer.save_pretrained(output_dir)"\n   ]\n  },\n  {\n   "cell_type": "code",\n   "execution_count": null,\n   "metadata": {},\n   "outputs": [],\n   "source": [\n    "from datasets import load_from_disk\n",\n    "\n",\n    "import zipfile\n",\n    "\n",\n    "zip_file_path = 'link to processed_dataset.zip'\n",\n    "extracted_folder_path = 'make a new dir'\n",\n    "\n",\n    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",\n    "    zip_ref.extractall(extracted_folder_path)\n",\n    "\n",\n    "processed_dataset = load_from_disk(extracted_folder_path)\n",\n    "processed_dataset"\n   ]\n  },\n  {\n   "cell_type": "code",\n   "execution_count": null,\n   "metadata": {},\n   "outputs": [],\n   "source": [\n    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",\n    "\n",\n    "os.mkdir('new dir path')\n",\n    "training_dir = "checkpoints dir"\n",\n    "\n",\n    "training_args = Seq2SeqTrainingArguments(\n",\n    "    predict_with_generate=True,\n",\n    "    evaluation_strategy="epoch",\n",\n    "    save_strategy = "epoch",\n",\n    "    num_train_epochs = 1,\n",\n    "    per_device_train_batch_size=4,\n",\n    "    per_device_eval_batch_size=4,\n",\n    "    output_dir=training_dir,\n",\n    ")"\n   ]\n  },\n  {\n   "cell_type": "code",\n   "execution_count": null,\n   "metadata": {},\n   "outputs": [],\n   "source": [\n    "import evaluate\n",\n    "metric = evaluate.load("rouge")"\n   ]\n  },\n  {\n   "cell_type": "code",\n   "execution_count": null,\n   "metadata": {},\n   "outputs": [],\n   "source": [\n    "import numpy as np\n",\n    "\n",\n    "ignore_pad_token_for_loss = True\n",\n    "\n",\n    "\n",\n    "def postprocess_text(preds, labels):\n",\n    "    preds = [pred.strip() for pred in preds]\n",\n    "    labels = [label.strip() for label in labels]\n",\n    "\n",\n    "    preds = ["\\n".join(nltk.sent_tokenize(pred)) for pred in preds]\n",\n    "    labels = ["\\n".join(nltk.sent_tokenize(label)) for label in labels]\n",\n    "\n",\n    "    return preds, labels\n",\n    "\n",\n    "\n",\n    "def compute_metrics(eval_preds):\n",\n    "    preds, labels = eval_preds\n",\n    "    if isinstance(preds, tuple):\n",\n    "        preds = preds[0]\n",\n    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",\n    "    if ignore_pad_token_for_loss:\n",\n    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",\n    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",\n    "\n",\n    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds,\n",\n    "                                                     decoded_labels)\n",\n    "\n",\n    "    result = metric.compute(predictions=decoded_preds,\n",\n    "                            references=decoded_labels,\n",\n    "                            use_stemmer=True)\n",\n    "    result = {k: round(v * 100, 4) for k, v in result.items()}\n",\n    "    prediction_lens = [\n",\n    "        np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds\n",\n    "    ]\n",\n    "    result["gen_len"] = np.mean(prediction_lens)\n",\n    "    return result"\n   ]\n  },\n  {\n   "cell_type": "code",\n   "execution_count": null,\n   "metadata": {},\n   "outputs": [],\n   "source": [\n    "from transformers import default_data_collator\n",\n    "\n",\n    "trainer = Seq2SeqTrainer(\n",\n    "    model=model,\n",\n    "    tokenizer=feature_extractor,\n",\n    "    args=training_args,\n",\n    "    compute_metrics=compute_metrics,\n",\n    "    train_dataset=processed_dataset['train'],\n",\n    "    eval_dataset=processed_dataset['validation'],\n",\n    "    data_collator=default_data_collator\n",\n    ")"\n   ]\n  },\n  {\n   "cell_type": "code",\n   "execution_count": null,\n   "metadata": {},\n   "outputs": [],\n   "source": [\n    "trainer.train(resume_from_checkpoint = True)"\n   ]\n  },\n  {\n   "cell_type": "code",\n   "execution_count": null,\n   "metadata": {},\n   "outputs": [],\n   "source": [\n    "trainer.save_model(training_dir)"\n   ]\n  },\n  {\n   "cell_type": "code",\n   "execution_count": null,\n   "metadata": {},\n   "outputs": [],\n   "source": [\n    "tokenizer.save_pretrained(training_dir)"\n   ]\n  },\n  {\n   "cell_type": "code",\n   "execution_count": null,\n   "metadata": {},\n   "outputs": [],\n   "source": [\n    "#to save checkpoint in drive\n",\n    "\n",\n    "import shutil\n",\n    "shutil.move("dir", "dir")"\n   ]\n  }\n ],\n "metadata": {\n  "kernelspec": {\n   "display_name": "Python 3",\n   "language": "python",\n   "name": "python3"\n  },\n  "language_info": {\n   "name": "python",\n   "version": "3.10.1"\n  }\n },\n "nbformat": 4,\n "nbformat_minor": 2\n}
